---
title: "$z^i_gz^a_g$ Tutorial"
author: "Ammon Thompson"
date: "March 14, 2020"
output:
# pdf_document: default
  html_document: default
#  pdf_format: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, eval = FALSE, include=FALSE}
options(tinytex.verbose = TRUE)
```

`r colorize <- function(x, color){
  
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, 
      x)
  } else x
  
}`

### Introduction
Zigzag computes the posterior probability each gene in a genome or gene set is actively expressed in a transcriptome. To compute these probabilities, zigzag uses Markov Chain Monte Carlo (MCMC) to sample from the posterior distribution of a heirarchical mixture model given relative expression estimates (*e.g.* TPM or FPKM) in a set of biological replicates (libraries). The manuscript describing this method is in press at *PNAS* ([preprint](https://www.biorxiv.org/content/10.1101/711630v2.abstract)). 

#### Model assumptions
Zigzag assumes the data follows a finite mixture model consisting of one inactive distribution and one or more active distributions. Each mixture component follows a Normal distribution on a log-scale, i.e. symmetric, bell-shaped distributions with low-expressed genes displaying dropout (zero reads) from binomial sampling error.


#### Summary of Analysis procedure

First, specify the finite mixture model by setting the number of mixture components. In our experience, in addition to the inactive mixture component, two active components are often necessary and sufficient to provide a good fit to the data. 

A nieve approach to selecting the number of active subcomponents is to first assume a simple, one active component mixture model, sample from the joint posterior distribution of model parameters and use posterior-predictive simulation to assess the adequacy of this model. If determined to be inadequate (simulated data do not resemble the real data) then specify a more complex model by adding more mixture components and reanalyzing. Repeat this until an adequate model is found or you nolonger see improvement in model fit. If you are unable to find a good enough fit, then the model assumed by zigzag may not be a good approximation of the processes that generated your data. 

To aid others as they interpret your results, include detailed reports of posterior sensitivity to prior assumptions and model fit/adaquacy using posterior-predictive simulation (see sections below).  

A key challenge inherent to fitting mixture models to data, is the non-identifiability of mixture component parameters---means, variances and weights. This is because the likelihood is invariant to the index of the component parameters. This is often referred to as the label switching problem. When all mixture components have identical prior distributions, the marginal posterior distributions for component parameters are identical and multi-modal. In theory, every gene would have a 0.5 probability of active expression. In practice, MCMCs behave poorly and often do not converge in a practical amount of time.

To mitigate this problem, identifiability constraints on parameters are necessary. We find a simple and effective way to impose identifiability constraints is to use truncated prior distributions that minimize the overlap of the support of each component mean. For each mean, an offset or threshold is set as a lower bound for active mixture components and an upper bound of the inactive mean prior distribution (see figure below).

To aid in setting these thresholds, you can inspect the empirical distribution of the data to specify component mean prior distributions. It is usually readily apparent where the modes of the latent mixture components are approximately located. By selecting prior thresholds between those modes you can impose identifiability on the model. This is an Empirical Bayesian approach because the data is used to specify the prior distribution. 

For example, in the plot below, the thick blue line on the left doubles as the upper boundary for the inactive mean prior distribution and the lower boundary for the first active mean prior distribution. The thin vertical red line on the far right shows the chosen lower boundary for a possible additional component of very highly expressed genes. This is sometimes necessary because Normal distributions can be sensitive to outliers. The dashed lines show the default Gamma(1, 1/3) prior densities for component means (scaled for visualization). 

```{r examplefig, fig.width=4, fig.height=3,fig.align="center", echo=F}
expression_data = read.table("example_lung.tpm", header = T, row.names = 1)
plot(density(log(expression_data[,1])), main ="", xlab = "log Expression", ylim = c(0, 0.25))
for(i in seq(ncol(expression_data))) lines(density(log(expression_data[,i])))

abline(v = c(1, 4), lwd = c(2, 1), col = c("blue", "red"))

lines(seq(0, -10, by= -.01)+1, dgamma(seq(0, 10, by = .01), 1, 1/3)/2, col = "blue", lty = 2)
lines(seq(0, 10, by =.01)+1, dgamma(seq(0, 10, by =.01), 1, 1/3)/2, col = "red", lty = 2)
lines(seq(0, 10, by =.01)+4, dgamma(seq(0, 10, by =.01), 1, 1/3)/2, col = "red", lty = 2)

```

Zigzag, by default, uses kernel density estimation to automatically select a model with one or two active components and set prior offsets or thresholds for all component means. In our experience, the range of the posterior distribution of the inactive mean is consistently below 1 log TPM and that of the active mean(s) is(are) consistently greater than -1 log TPM. This may not be true for all data sets. We find this works well as a starting point, but often a visual inspection of densities using basic R functions, such as *density*, can allow users to set priors that yield robust posterior inferences. 

A fully Bayesian analysis would entail setting informative priors based on prior knowledge about the number and location of mixture component parameters. We expect such prior knowledge will rarely be detailed enough to be useful and that an empirical Bayesian approach with model refinement through model checking and sensitivity analysis would produce robust inferences.

In the tutorial below we will analyze a small dataset of expression level estimates in units of TPM downloaded from the [GTEx](https://www.gtexportal.org/home/) databse. This data is included in the zigzag repository. 

The tutorial describes the following steps for conducting a proper Bayesian analysis with zigzag:

1) MCMC: Burnin-in.
2) MCMC: Simulate draws from model posterior.
3) Check MCMC quality. 
4) Check posterior sensitivity and adequacy.
5) Analyze and interpret posterior sample.


### Install zigzag
Navigate to a directory where you would like to install the zigzag repository. 

```{bash, eval = FALSE}
bash
git clone https://github.com/ammonthompson/zigzag.git
```
To install the R package, open an R terminal or in the console in Rstudio. If your working directory is not where the zigzag package is, then include the path to zigzag. The package is a directory that contains the DESCRIPTION file.
```{r, eval=FALSE}
R
install.packages("zigzag", type = "source", repos = NULL)
library(zigzag)
```
Note, zigzag depends on R packages coda and matrixStats.



### Load Expression and Gene Length Data into R
We have provided example data for this tutorial. These are subsets of genes used in the zigzag publication. The expression data is found in the quick_start_guide directory. These files contain the TPM levels of 5,000 genes estimated in 4 human lung RNA-seq libraries from the GTEx dataset. 

After cloning the zigzag repository and installing the zigzag R package, open R and set the quick_start_guide directory as your working directory.

You can copy-and-paste the R code to run analyses.

```{r, echo = TRUE}
expression_data = read.table("example_lung.tpm", header = T, row.names = 1)

head(expression_data)
```

zigzag requires the data contain row names which are gene names. Ensure the row names correspond in the gene length and expression files.

```{r,}
human_gene_lengths = read.table("example_gene_length.txt", header = T, row.names = 1)

head(round(human_gene_lengths, digits = 1))
```


Here is what the log TPM distribution looks like for the four replicate transcriptomes:
```{r figthresh, fig.width=5, fig.height=4,fig.align="center", echo=1:2}
plot(density(log(expression_data[,1])), main ="", xlab = "log Expression", ylim = c(0, 0.25))
for(i in seq(ncol(expression_data))) lines(density(log(expression_data[,i])))
```

#### Calculate gene lengths
When you analyze your own data, you will need to obtain mean transcript lengths for each gene. There are many tools available to do this such as [*GTFtools*](http://www.genemine.org/gtftools.php). In the *scripts* directory, we have provided a bash script called *compute_gene_transcripts_length.sh* which calculates the average length of transcripts belonging to each gene from a gtf/gff annotation file. Best to run it on a computer with many cores. It probably isn't the fastest tool for this job. To use, ensure *Rscript* is installed on your system. You will also need to know the tags for gene id and transcript id in column 9 of the gtf file. Column 9 is the semicolon-delimmeted attributes column for each feature (*e.g.* exon, CDS, gene, etc.). Usually, "gene_id" and "transcript_id" are the tags. For example the attributes tags for genes and transcripts (red text) for an exon appear as: 

`r colorize("gene_id", "red")` "FBgn0052816"**;** gene_symbol "CG32816"**;** `r colorize("transcript_id", "red")` "FBtr0070102"**;** transcript_symbol "CG32816-RA"**;**

For help type:  ./compute_gene_transcripts_length.sh -h

Here is an example command:

```{bash, eval = F, echo = T}
./compute_gene_transcripts_length.sh -g gene_id -t transcript_id -c 8 -f annotation.gtf
```
This will take several hours to run and will output two files, a transcript length file and a mean transcript length file for each gene.
Ensure that your gene lengths table has the same gene order as your gene expression table before instantiating a zigzag object.



### Load data into a zigzag object and set hyperparameters
Once you have decided on a mixture model, create a zigzag object. This object contains all the parameters in the mixture model as well as the functions for generating and analyzing samples from the posterior distribution.

zigzag instances are Refclass objects. Their member variables and methods are called with the \$ symbol. Methods and variables are called with the following notation: object\$method(), or object\$variable. A new zigzag object is created this way:

```{r, eval=FALSE}
my_zigzag =zigzag$new(data and hyperprior settings)
```

Load the expression data and gene length data into a zigzag object. Specify a mixture model that contains two subdistributions in the active expression component with \textit{lower} boundaries set at 1 and 4 by setting threshold_a = c(1, 4) and threshold_i = 1. The \textit{upper} boundary for the inactive mean prior is by default equal to the threshold_a[1], in this case, 1. For all other priors we will keep the default settings. If num_active_components is not set, or set to "auto", zigzag will use kernel density estimation to determine whether one or two active subcomponents are detected in the data. 

If threshold_a is not set, or is set to "auto", thresholds will be estimated from the data based on the number of active components specified. Zigzag uses the output_directory variable to create a directory where all output files will be written. To see all zigzag variables and default settings including hyperpriors type ?zigzag in the Rstudio console.

```{r, , eval=FALSE}
my_zigzag = zigzag$new(data = expression_data, gene_length = human_gene_lengths, 
                       output_directory = "my_zigzag_output", num_active_components = 2,
                       threshold_i = 1, threshold_a = c(1, 4))
```

The minimum command for making a zigzag object is:
```{r, eval = F, echo = T}
my_zigzag = zigzag$new(expression_data, human_gene_lengths)
```
WIth this command zigzag automatically uses the data to specify the mixture model and prior distributions and sets all genes to have equal mean transcript length.

When the zigzag object is created, the hyperprior settings will be summarized in a file in your output_directory, in this case my_zigzag_output/hyperparameter_settings.txt

You should create at the very least two zigzag objects and run independent MCMCs for each. 


### Run burnin and assess convergence with log files
Use the burnin function to run an initial burnin. Here we will run the analysis for 20,000 generations sampling every 50 generations of the chain.
If write_to_files = TRUE, the burnin sample will, by default, be written to files in a directory called my_zigzag_output/output_burnin. Type ?burnin for more details.

```{r,, eval = FALSE}
my_zigzag$burnin(sample_frequency = 50, ngen=20000, write_to_files = TRUE)
```


Evaluate the burnin log files to determine if the chain has converged to the stationary distribution. The plot of sampled parameter values should randomly vary around a constant. You can look at output files for a burnin I ran located in "example_zigzag_output":

```{r, eval = TRUE, echo = TRUE}

burn = read.table("example_zigzag_output/output_burnin/output_model_parameters.log", 
                  header = T, row.names =1)
```

```{r, eval = FALSE}

for(i in seq(ncol(burn))) plot(burn[,i], type = "l", main = colnames(burn)[i])

```
For example, the variable $s_0$ appears to have converged on the stationary distribution around -2:

```{r s0fig_burn, echo = TRUE, fig.width=5, fig.height=4,fig.align="center"}
plot(burn[,2], type = "l", main = colnames(burn)[2])
```

If the chain has not converged, then simply call burnin again setting ngen to a higher number. The burnin will proceed from the generation you left off. New log files will be created unless you set the append parameter = TRUE. 

If it becomes clear that some prior distributions are too informative and are having a strong influence on the posterior distribution, then increase the variance of the prior distribution and/or extend the boundaries of of the prior support to minimize its influence on the posterior distribution.

If satisfied that all parameters have converged, proceed to run the mcmc. Type ?mcmc for more details.


### Run MCMC

To sample from the posterior distribution, use the mcmc function. To run posterior-predictive simulation during the mcmc, set run_posterior_predictive = TRUE.

```{r,, eval = FALSE}
my_zigzag$mcmc(sample_frequency = 50, ngen = 50000, run_posterior_predictive = TRUE, mcmcprefix = "my_lung")
```



### Analyze MCMC output files
When the MCMC completes, zigzag generates a number of mcmc summary files, plots, and diagnostics which are placed in a subdirectory within the mcmc directory called "mcmc_report".

MCMC posterior sample files and posterior-predictive simulation files from an MCMC I previously ran for 750,000 generations are located in the directory: example_zigzag_output/example_lung_mcmc_output/ 

To examine the quality of your MCMC and measure effective sample size, load the example_lung_model_parameters.log into a dataframe.
```{r, eval = TRUE, echo = TRUE}
post = read.table("example_zigzag_output/example_lung_mcmc_output/example_lung_model_parameters.log", 
                  header = T, row.names = 1)
```

To plot all parameter mcmc chains:
```{r, eval = FALSE}
for(i in seq(ncol(post))) plot(post[,i], type = "l", main = colnames(post)[i])

```

For example, $s_0$ doesn't appear to shift or drift from a stable distribution around -2:
```{r s0fig_post, eval = TRUE, echo = TRUE, fig.width=5, fig.height=4,fig.align="center"}

plot(post$s0, type = "l", main = colnames(post)[2])

```
Pdf files with trace plots of model parameters---except gene-specific parameters---are generated when the MCMC is complete and placed in the mcmc_report directory. 


Because samples from an MCMC are auto-correlated to varying degrees, to estimate sample size for each parameter, it is necessary to correct for autocorrelation. To calculate the effective sample size (ESS) of each parameter use the *effectiveSize* function from the package *coda*. For example, $s_0$ has an ESS of:
```{r,eval = TRUE, echo = TRUE}
library(coda)
effectiveSize(post)[2]
```
Effective sample size estimates for all parameters are also generated and placed in mcmc_report directory when the MCMC is complete.


According to my zigzag analysis, here are the posterior distributions of the three mixture component means (scaled by 0.02 for visualization) relative to the empirical distributions of log expression:

```{r post_meansfig_overlay, fig.width = 6, fig.height = 4, fig.align="center", eval = TRUE, echo = FALSE}
plot(NULL, main ="", xlab = "log Expression", xlim = c(-8, 12), ylim = c(0, 0.25))
for(i in seq(ncol(expression_data))) lines(density(log(expression_data[,i])))

di = density(post$inactive_mean)
da1 = density(post$active_mean_component1)
da2 = density(post$active_mean_component2)
lines(di$x, 0.02 * di$y, type = "s", col = "blue")
lines(da1$x, 0.02 * da1$y, type = "s", col = "red")
lines(da2$x, 0.02 * da2$y, type = "s", col = "red")

```

Confirm all independent MCMCs converged on the same stationary distribution for all parameters. MCMC files can be compared visually using [Tracer](http://tree.bio.ed.ac.uk/software/tracer/). Also, check that all runs produce the same estimates of the probability of active expression for all genes. If you see a significant difference for some genes, that indicates those genes didn't converge. Check the mcmc traces for $Y_g$ and $\sigma^2_g$ for those genes. 


### Sensitivity Analysis: How sensitive is the posterior to the prior?

Sensitivity analysis explores the sensitivity of the posterior distribution to prior specification by comparing posterior distributions to the prior distributions under different priors. 

The plots below show an example of an analysis exploring posterior sensitivity to the choice of component mean prior thresholds (threshold_i and threshold_a). For this tutorial, several different analyeses of the example dataset were done with different thresholds separating the prior distributions for the inactive mean and the first active component mean with threshold_i = threashold_a (blue vertical line; below).

Thresholds targeting the region between the left shoulder and the central mode of the data distributions, ranging from -1 to 2 produce posterior distributions that are virtually identical and have little to no density at the prior threshold.
```{r fig3, echo = F, fig.height = 5, fig.width = 8, fig.align = "center"}

#Load posterior data
i_post = read.table("threshold_sensitivity_results/inactiveMean_threshold_experiments.log", header = T, row.names = 1)
a_post = read.table("threshold_sensitivity_results/activeMean_threshold_experiments.log", header = T, row.names = 1)



opar = par("mai")

par("mai" = opar * 0.35)

breaks = seq(-2.5, 3.2, by = 0.05)

# Intermediate prior threshold

layout(matrix(seq(12), ncol = 3, byrow = T))

tt = c(-1, 0, 1, 2)

for(i in c(4:7)){
  plot(density(log(expression_data[,1])), main ="", xlab = "log Expression", xlim = c(-6, 8), ylim = c(0, 0.25))
  for(j in seq(ncol(expression_data))) lines(density(log(expression_data[,j])))
  
  abline(v = c(tt[i-3]), lwd = 2, col = c("blue"))
  
  lines(seq(0, -10, by= -.01)+tt[i-3], dgamma(seq(0, 10, by = .01), 1, 1/3)/2, col = "blue", lty = 2)
  lines(seq(0, 10, by =.01)+tt[i-3], dgamma(seq(0, 10, by =.01), 1, 1/3)/2, col = "red", lty = 2)
  
  
  hist(i_post[,i], breaks = breaks, xlim = c(-2.5, 2), freq = F, border = "blue", main ="", xlab = "log Expression", axes = F)
  axis(1)
  abline(v=tt[i-3], lwd = 2, col = "blue")
  lines(seq(0, -10, by= -.01)+tt[i-3], 5 * dgamma(seq(0, 10, by = .01), 1, 1/3)/2, col = "blue", lty = 2)
  
  hist(a_post[,i], breaks = breaks, xlim = c(-1,3.2), freq = F, border = "red", main ="", xlab = "log Expression", axes = F)
  axis(1)
  abline(v=tt[i-3], lwd = 2, col = "blue")
  lines(seq(0, 10, by =.01)+tt[i-3], 20 * dgamma(seq(0, 10, by =.01), 1, 1/3)/2, col = "red", lty = 2)
}

par("mai" = opar)

```

When the prior threshold is set to a value above the mode of the expression distribution---around 3 log TPM---the posterior becomes very sensitive to the prior.  Below the prior threshold was set at a high value to the right of the distribution mode. Under this prior, the posterior distribution is highly concentrated at the threshold (right; red histogram).
```{r fig2, echo = F, fig.height = 1.25, fig.width = 8, fig.align = "center"}
opar = par("mai")

par("mai" = opar * 0.35)

# extreme right prior threshold

layout(matrix(seq(3), ncol = 3))
plot(density(log(expression_data[,1])), main ="", xlab = "log Expression", ylim = c(0, 0.25), xlim = c(-6,8))
for(i in seq(ncol(expression_data))) lines(density(log(expression_data[,i])))

abline(v = 4, lwd = 2, col = c("blue"))

lines(seq(0, -10, by= -.01)+4, dgamma(seq(0, 10, by = .01), 1, 1/3)/2, col = "blue", lty = 2)
lines(seq(0, 10, by =.01)+4, dgamma(seq(0, 10, by =.01), 1, 1/3)/2, col = "red", lty = 2)


hist(i_post[,9], breaks = 20, xlim = c(0.5, 4), freq = F, border = "blue", main ="", xlab = "log Expression", axes = F)
axis(1)
abline(v=4, lwd = 2, col = "blue")
lines(seq(0, -10, by= -.01)+4, 20 * dgamma(seq(0, 10, by = .01), 1, 1/3)/2, col = "blue", lty = 2)

hist(a_post[,9], breaks = 20, xlim = c(4, 4.05), freq = F, border = "red", main ="", xlab = "log Expression", axes = F)
axis(1)
abline(v=4, lwd = 2, col = "blue")
lines(seq(0, 10, by =.01)+4, 500 * dgamma(seq(0, 10, by =.01), 1, 1/3)/2, col = "red", lty = 2)

par("mai" = opar)
```


Likewise, when assuming a prior threshold to the extreme left, all the posterior sample for the inactive mean is concentrated very close to the upper threshold (middle; blue historgram) indicating the prior threshold is strongly influencing the posterior distribution.
```{r fig1, echo = F, fig.height = 1.25, fig.width = 8, fig.align = "center"}

opar = par("mai")

par("mai" = opar * 0.35)
# Far left prior threshold

layout(matrix(seq(3), ncol = 3))
plot(density(log(expression_data[,1])), main ="", xlab = "log Expression", ylim = c(0, 0.25), xlim = c(-6,8))
for(i in seq(ncol(expression_data))) lines(density(log(expression_data[,i])))

abline(v = c(-3), lwd = 2, col = c("blue"))

lines(seq(0, -10, by= -.01)-3, dgamma(seq(0, 10, by = .01), 1, 1/3)/2, col = "blue", lty = 2)
lines(seq(0, 10, by =.01)-3, dgamma(seq(0, 10, by =.01), 1, 1/3)/2, col = "red", lty = 2)


hist(i_post[,2], breaks = 20, xlim = c(-3.2, -2.99), freq = F, border = "blue", main ="", xlab = "log Expression", axes = F)
axis(1)
abline(v=-3, lwd = 2, col = "blue")
lines(seq(0, -10, by= -.01)-3, 100 * dgamma(seq(0, 10, by = .01), 1, 1/3)/2, col = "blue", lty = 2)

hist(a_post[,2], breaks = 20, xlim = c(-3, 3.2), freq = F, border = "red", main ="", xlab = "log Expression", axes = F)
axis(1)
abline(v=-3, lwd = 2, col = "blue")
lines(seq(0, 10, by =.01)-3, 20 * dgamma(seq(0, 10, by =.01), 1, 1/3)/2, col = "red", lty = 2)
par("mai" = opar)
```

For this dataset, any threshold between -0.5 log TPM and +2.5 log TPM yields the same posterior distribution because the data strongly suggest the two means lie on opposite ends of this interval (shaded in gray below). Any threshold placed within the gray interval will produce robust posterior inferences. Zigzag's automatic threshold finding option often does a good job placing thresholds in this region when a new zigzag object is created.


```{r fig4, echo = F, fig.height = 3, fig.width = 5, fig.align = "center"}
layout(1)
plot(density(log(expression_data[,1])), main ="", xlab = "log Expression", ylim = c(0, 0.25), xlim = c(-6,8))
for(i in seq(ncol(expression_data))) lines(density(log(expression_data[,i])))

polygon(c(-0.75,2.65,2.65,-0.75), c(-100,-100,100,100), col = rgb(0,0,0,0.1), border = NA)
```

Generally, when the boundary of a prior distribution is strongly influencing the posterior distribution, a significant amount of the posterior density will be concentrated at that boundary. Simply inspecting the posterior distribution will illuminate whether this is the case. Prior distributions with small variances or boundaries that conflict with the likelihood will lead to posterior distributions that are sensitive to the prior distribution. 

Pdf plots of posterior distributions relative to prior distributions are also automatically generated and placed in the mcmc_report directory when the MCMC is complete. If the prior is relatively flat over the range of the posterior distribution then you can be confident that the prior is not strongly influencing the posterior.



*Tracer is also a good program for viewing mcmc files: [Tracer download page](http://tree.bio.ed.ac.uk/software/tracer/)


### Assess model adequacy with posterior-predictive simulation

Model-based inference is based on the premise that our inference model provides an adequate description of the process that gave rise to our data. The Bayesian approach for assessing model adequacy is called posterior-predictive assessment ([Gelman et al. 1996](http://www.stat.columbia.edu/~gelman/research/published/A6n41.pdf)). This approach is based on the following idea: if the inference model is a good approximation of the process that gave rise to our observations, then we should be able to use the posterior distribution of that model to simulate datasets that resemble the dataset we used to infer the model posterior distribution. 

To compare real data to simulated data we use a summary statistic to quantify the resemblance between the datasets. By repeatedly simulating datasets from the joint posterior distribution of the model parameters, we can construct a predictive distribution of summary statistics and compute the difference between the statistic computed from the simulated and from the observed data. A difference close to zero indicate we are able to infer a model that generates similar data.

When running the mcmc function, if run_posterior_predictive = TRUE, zigzag will periodically sample from the mcmc chain to simulate data and measure three discrepancy statistics: (1) the lower-level Wasserstein statistic, which measures the discrepancy between the cumulative distributions of the observed expression levels for detected genes, $X$, and the expected expression levels for detected genes given the model parameters, $E[X|\theta]$; (2) the upper-level Wasserstein statistic measures the discrepancy between the sampled posterior true expression levels $Y$ and the distribution of simulated values of $Y$ drawn from the posterior distribution of the upper-level of the model, $\theta_2$, and; (3) the Rumsfeld statistic, which measures the discrepancy between the fraction of undetected transcripts averaged over libraries and the expected fraction of undetected transcripts given the model parameters. See section S1.5 in the Supporting information of the article for a more detailed description.

Posterior-predictive distributions of discrepancy statistics can be used to compare models with different prior thresholds for means and variance parameters and different numbers of active components. For example, posterior-predictive simulations drawn during MCMC's for the sensitivity analysis above indicate that the thresholds that strongly influence the posterior distributions, also decrease the model fit to the data: the posterior-predicitve distributions tend to diverge from zero the more the proir thresholds move to the extreme tails of the empirical distributions.


```{r fig5, echo = F, fig.height = 4, fig.width = 7, fig.align = "center"}
layout(1)

Wu = read.table("threshold_sensitivity_results/W_U_threshold_exp.log", header = T,row.names = 1)
Wl = read.table("threshold_sensitivity_results/W_L_threshold_exp.log", header = T, row.names = 1)

thresholds = c(-4, -3, -2, -1, 0, 1, 2, 3, 4, 5)

layout(matrix(seq(2), ncol = 2))
boxplot(Wu, names = thresholds, outline = F, main = "Upper-Level Wasserstein Dist.", xlab = "prior threshold position")
abline(h=0, lty = 2)
polygon(c(3.5, 7.5, 7.5, 3.5), c(-10,-10,10,10), col = rgb(0,0,0,0.1), border = NA)

boxplot(Wl, names = thresholds, outline = F, main = "Lower-Level Wasserstein Dist.", xlab = "prior threshold position")
abline(h=0, lty = 2)
polygon(c(3.5, 7.5, 7.5, 3.5), c(-10,-10,10,10), col = rgb(0,0,0,0.1), border = NA)
```


Similarly, posterior-predictive simulation can be used to assess the relative fit of models with different numbers of active mixture components. If we compare the posterior-predictive distributions generated from a one component and two component posterior distribution, we see that the two component predictive interval is more centered on zero (higher tail-area probability). This suggests a two-component model provides an improvement in fit for the example dataset:

```{r ,eval = TRUE, echo = TRUE}
post_1comp = read.table("example_zigzag_output/example_lung_mcmc_output/example_1comp_lung.post_predictive_output.log", header = T, row.names = 1)

post_2comp = read.table("example_zigzag_output/example_lung_mcmc_output/example_lung.post_predictive_output.log", header = T, row.names = 1)

```

```{r fig_pp1,eval = TRUE, echo = F, fig.height = 2.5, fig.width=6.5, fig.align="center"}

layout(matrix(seq(2), ncol = 2))
opar = par("mai")
par("mai" = opar * 0.5)

boxplot(post_1comp, outline = FALSE, main = "one-component", names = c("W_lower", "W_upper", "Rums"))
abline(h=0, col = "red")

boxplot(post_2comp, outline = FALSE, main = "two-component", names = c("W_lower", "W_upper", "Rums"))
abline(h=0, col = "Red")

par("mai" = opar)

```



### RESULTS: The posterior probability each gene is active
This is found in the file: example_lung_probability_active.tab. If more than two active components are specified in the model, then the file example_lung_probability_in_component.tab gives the probability each gene is in each of component in the model. The plot below shows the probabilities for all 5000 genes (scaled by 0.25 for visualization; red) in relation to the mean log-expression level of the data.

```{r,,echo = TRUE}
prob_active = read.table("example_zigzag_output/example_lung_mcmc_output/example_lung_probability_active.tab",
                         header = T, row.names = 1)

head(round(cbind(expression_data, prob_active), digits = 3))

```


```{r, fig.width = 6, fig.height = 4, fig.align="center",echo =FALSE}
plot(density(log(expression_data[,1])), main ="", xlab = "log Expression", ylim = c(0, 0.25), axes = FALSE)
axis(1)

points(log(rowMeans(expression_data)), 0.25 * prob_active$prob_active, col = "red", cex = 0.5)

for(i in seq(ncol(expression_data))) lines(density(log(expression_data[,i])))

```




